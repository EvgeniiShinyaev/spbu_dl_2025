{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68022fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3c53756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54930825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    path_train_x: str = \"train_x.csv\"\n",
    "    path_train_y: str = \"train_y.csv\"\n",
    "    path_test_x: str  = \"test_x.csv\"\n",
    "\n",
    "    test_size: float = 0.2\n",
    "    batch_size: int = 256\n",
    "    epochs: int = 100\n",
    "    lr: float = 1.5e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    pct_start: float = 0.2\n",
    "    max_grad_norm: float = 1.0\n",
    "    patience: int = 20\n",
    "    seed: int = 42\n",
    "\n",
    "cfg = CFG()\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ceca0c",
   "metadata": {},
   "source": [
    "Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89b435d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133081</th>\n",
       "      <td>37.47518</td>\n",
       "      <td>-14.34414</td>\n",
       "      <td>40.54872</td>\n",
       "      <td>-9.10171</td>\n",
       "      <td>16.77175</td>\n",
       "      <td>-17.77113</td>\n",
       "      <td>-16.50156</td>\n",
       "      <td>-4.09543</td>\n",
       "      <td>2.49723</td>\n",
       "      <td>-0.46428</td>\n",
       "      <td>...</td>\n",
       "      <td>104.03557</td>\n",
       "      <td>-115.62803</td>\n",
       "      <td>-13.79660</td>\n",
       "      <td>31.60436</td>\n",
       "      <td>28.95927</td>\n",
       "      <td>-25.93164</td>\n",
       "      <td>67.64670</td>\n",
       "      <td>-25.76691</td>\n",
       "      <td>-81.90373</td>\n",
       "      <td>-61.48682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111484</th>\n",
       "      <td>48.17393</td>\n",
       "      <td>-7.02208</td>\n",
       "      <td>-30.36086</td>\n",
       "      <td>-2.41924</td>\n",
       "      <td>2.15406</td>\n",
       "      <td>-8.44502</td>\n",
       "      <td>-1.68191</td>\n",
       "      <td>-8.71434</td>\n",
       "      <td>-7.83802</td>\n",
       "      <td>-5.58019</td>\n",
       "      <td>...</td>\n",
       "      <td>11.58664</td>\n",
       "      <td>24.58950</td>\n",
       "      <td>-36.95682</td>\n",
       "      <td>0.73922</td>\n",
       "      <td>-0.06330</td>\n",
       "      <td>53.75838</td>\n",
       "      <td>-81.05330</td>\n",
       "      <td>8.42811</td>\n",
       "      <td>12.08694</td>\n",
       "      <td>-1.91676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448402</th>\n",
       "      <td>36.58141</td>\n",
       "      <td>26.03203</td>\n",
       "      <td>-4.92774</td>\n",
       "      <td>35.71620</td>\n",
       "      <td>8.53080</td>\n",
       "      <td>3.73167</td>\n",
       "      <td>-7.98443</td>\n",
       "      <td>-7.43976</td>\n",
       "      <td>-1.69797</td>\n",
       "      <td>10.75028</td>\n",
       "      <td>...</td>\n",
       "      <td>23.61190</td>\n",
       "      <td>-366.07968</td>\n",
       "      <td>-62.38201</td>\n",
       "      <td>113.48188</td>\n",
       "      <td>4.72741</td>\n",
       "      <td>181.64459</td>\n",
       "      <td>-134.46216</td>\n",
       "      <td>8.50795</td>\n",
       "      <td>94.15573</td>\n",
       "      <td>-8.47276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254414</th>\n",
       "      <td>41.15615</td>\n",
       "      <td>-17.77029</td>\n",
       "      <td>-32.30961</td>\n",
       "      <td>-21.03778</td>\n",
       "      <td>12.80330</td>\n",
       "      <td>-13.48031</td>\n",
       "      <td>-3.14951</td>\n",
       "      <td>-7.62647</td>\n",
       "      <td>-4.48901</td>\n",
       "      <td>-4.29075</td>\n",
       "      <td>...</td>\n",
       "      <td>25.11398</td>\n",
       "      <td>-79.64532</td>\n",
       "      <td>-77.08169</td>\n",
       "      <td>38.88094</td>\n",
       "      <td>28.52025</td>\n",
       "      <td>24.17783</td>\n",
       "      <td>-86.62542</td>\n",
       "      <td>-1.19418</td>\n",
       "      <td>-74.73449</td>\n",
       "      <td>-17.28130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272580</th>\n",
       "      <td>40.54855</td>\n",
       "      <td>78.77563</td>\n",
       "      <td>-23.29877</td>\n",
       "      <td>98.60192</td>\n",
       "      <td>-30.11496</td>\n",
       "      <td>26.94220</td>\n",
       "      <td>-8.87771</td>\n",
       "      <td>-3.23280</td>\n",
       "      <td>-1.04841</td>\n",
       "      <td>31.69655</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.89388</td>\n",
       "      <td>-252.61021</td>\n",
       "      <td>118.93768</td>\n",
       "      <td>-155.87390</td>\n",
       "      <td>51.85666</td>\n",
       "      <td>-365.15815</td>\n",
       "      <td>59.34936</td>\n",
       "      <td>52.47311</td>\n",
       "      <td>99.00695</td>\n",
       "      <td>-10.18840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "133081  37.47518 -14.34414  40.54872  -9.10171  16.77175 -17.77113 -16.50156   \n",
       "111484  48.17393  -7.02208 -30.36086  -2.41924   2.15406  -8.44502  -1.68191   \n",
       "448402  36.58141  26.03203  -4.92774  35.71620   8.53080   3.73167  -7.98443   \n",
       "254414  41.15615 -17.77029 -32.30961 -21.03778  12.80330 -13.48031  -3.14951   \n",
       "272580  40.54855  78.77563 -23.29877  98.60192 -30.11496  26.94220  -8.87771   \n",
       "\n",
       "              7        8         9  ...         80         81         82  \\\n",
       "133081 -4.09543  2.49723  -0.46428  ...  104.03557 -115.62803  -13.79660   \n",
       "111484 -8.71434 -7.83802  -5.58019  ...   11.58664   24.58950  -36.95682   \n",
       "448402 -7.43976 -1.69797  10.75028  ...   23.61190 -366.07968  -62.38201   \n",
       "254414 -7.62647 -4.48901  -4.29075  ...   25.11398  -79.64532  -77.08169   \n",
       "272580 -3.23280 -1.04841  31.69655  ...   -9.89388 -252.61021  118.93768   \n",
       "\n",
       "               83        84         85         86        87        88  \\\n",
       "133081   31.60436  28.95927  -25.93164   67.64670 -25.76691 -81.90373   \n",
       "111484    0.73922  -0.06330   53.75838  -81.05330   8.42811  12.08694   \n",
       "448402  113.48188   4.72741  181.64459 -134.46216   8.50795  94.15573   \n",
       "254414   38.88094  28.52025   24.17783  -86.62542  -1.19418 -74.73449   \n",
       "272580 -155.87390  51.85666 -365.15815   59.34936  52.47311  99.00695   \n",
       "\n",
       "              89  \n",
       "133081 -61.48682  \n",
       "111484  -1.91676  \n",
       "448402  -8.47276  \n",
       "254414 -17.28130  \n",
       "272580 -10.18840  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv(cfg.path_train_x, index_col=0)\n",
    "y_train = pd.read_csv(cfg.path_train_y, index_col=0)['year']\n",
    "X_test  = pd.read_csv(cfg.path_test_x).set_index(\"id\")\n",
    "\n",
    "assert X_train.shape[1] == X_test.shape[1], \"train/test feature mismatch\"\n",
    "\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8daa91a",
   "metadata": {},
   "source": [
    "Трейн/валидация + масштабирование\n",
    "Разделение train на обучающую и валидационную выборки (train_test_split). Масштабирование признаков X стандартным скейлером (StandardScaler) и отдельное масштабирование целевой переменной y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fed2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_df, X_val_df, y_tr_ser, y_val_ser = train_test_split(\n",
    "    X_train, y_train, test_size=cfg.test_size, random_state=cfg.seed\n",
    ")\n",
    "\n",
    "# scalers\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "X_tr = x_scaler.fit_transform(X_tr_df.values)\n",
    "X_val = x_scaler.transform(X_val_df.values)\n",
    "X_te  = x_scaler.transform(X_test.values)\n",
    "\n",
    "y_tr = y_scaler.fit_transform(y_tr_ser.values.reshape(-1, 1)).ravel()\n",
    "y_val = y_scaler.transform(y_val_ser.values.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520736a5",
   "metadata": {},
   "source": [
    "Датасет для PyTorch\n",
    "Определяется класс SongsDS (наследует torch.utils.data.Dataset): возвращает пары (X, y) для батчевой загрузки. Это обёртка над numpy-массивами/тензорами для использования с DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e87a5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongsDS(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray | None = None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = None if y is None else y.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.y is None:\n",
    "            return torch.from_numpy(self.X[i])\n",
    "        return torch.from_numpy(self.X[i]), torch.from_numpy(np.array(self.y[i]))\n",
    "\n",
    "ds_tr  = SongsDS(X_tr, y_tr)\n",
    "ds_val = SongsDS(X_val, y_val)\n",
    "ds_te  = SongsDS(X_te, None)\n",
    "\n",
    "dl_tr  = DataLoader(ds_tr,  batch_size=cfg.batch_size, shuffle=True,  num_workers=0, drop_last=False)\n",
    "dl_val = DataLoader(ds_val, batch_size=cfg.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "dl_te  = DataLoader(ds_te,  batch_size=cfg.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9f206",
   "metadata": {},
   "source": [
    "Архитектура модели (Residual MLP)\n",
    "ResidualBlock — блок с пропуском (skip-connection) и нелинейностью. ResidualMLP — многослойная полносвязная сеть с несколькими такими блоками, на выходе один нейрон (регрессия года)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "463f7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim: int, p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.net(x)\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        super().__init__()\n",
    "        widths = [in_dim, 256, 128, 64]\n",
    "        layers = []\n",
    "        layers += [nn.Linear(widths[0], widths[1]), nn.LayerNorm(widths[1]), nn.GELU(), nn.Dropout(0.15)]\n",
    "        layers += [ResidualBlock(widths[1], p=0.10)]\n",
    "        layers += [nn.Linear(widths[1], widths[2]), nn.LayerNorm(widths[2]), nn.GELU(), nn.Dropout(0.15)]\n",
    "        layers += [ResidualBlock(widths[2], p=0.10)]\n",
    "        layers += [nn.Linear(widths[2], widths[3]), nn.LayerNorm(widths[3]), nn.GELU(), nn.Dropout(0.10)]\n",
    "        layers += [nn.Linear(widths[3], 1)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self._init_weights_xavier()\n",
    "\n",
    "    def _init_weights_xavier(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b313c222",
   "metadata": {},
   "source": [
    "Оптимизатор SAM\n",
    "Реализация SAM (Sharpness-Aware Minimization) как обёртки над базовым оптимизатором (AdamW): первый проход делает «выпуклую» шаговую оценку по направлению увеличения лосса, второй — обычный шаг базового оптимизатора. Это помогает находить более плоские минимумы и лучше обобщать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=True, **kwargs):\n",
    "        assert rho >= 0.0\n",
    "        if not isinstance(base_optimizer, type):\n",
    "            raise ValueError(\"base_optimizer должен быть классом, а не инстансом.\")\n",
    "        self.base_optimizer = base_optimizer(params, **kwargs)\n",
    "        self.rho = rho\n",
    "        self.adaptive = adaptive\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super().__init__(self.base_optimizer.param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad: bool = True):\n",
    "        eps = 1e-12\n",
    "        for group in self.base_optimizer.param_groups:\n",
    "            scale = self.rho / (self._grad_norm(group) + eps)\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                e_w = (torch.pow(p, 2) if self.adaptive else 1.0) * p.grad * scale\n",
    "                self.state[p]['e_w'] = e_w\n",
    "                p.add_(e_w)\n",
    "        if zero_grad:\n",
    "            self.base_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad: bool = True):\n",
    "        for group in self.base_optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.sub_(self.state[p]['e_w'])\n",
    "        self.base_optimizer.step()\n",
    "        if zero_grad:\n",
    "            self.base_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError(\"SAM: вызывай first_step/second_step явно.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _grad_norm(group):\n",
    "        norms = []\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            g = p.grad.coalesce().values() if p.grad.is_sparse else p.grad\n",
    "            scale = torch.abs(p) if group.get('adaptive', True) else 1.0\n",
    "            norms.append(torch.norm(scale * g))\n",
    "        if not norms:\n",
    "            # на случай пустых градиентов\n",
    "            device = group['params'][0].device\n",
    "            return torch.tensor(0., device=device)\n",
    "        return torch.norm(torch.stack(norms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d64b4",
   "metadata": {},
   "source": [
    "Оценка MSE в реальном масштабе\n",
    "evaluate_mse_real_scale считает MSE в исходном масштабе целевой: предсказания модели из валидатора преобразуются обратно через y_scaler.inverse_transform, затем mean_squared_error(y_true, y_pred)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7139ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mse_real_scale(model: nn.Module, dl: DataLoader, y_scaler: StandardScaler) -> float:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            yhat = model(xb).detach().cpu().numpy()\n",
    "            yb_np = yb.detach().cpu().numpy()\n",
    "            yhat_real = y_scaler.inverse_transform(yhat.reshape(-1, 1)).ravel()\n",
    "            y_real    = y_scaler.inverse_transform(yb_np.reshape(-1, 1)).ravel()\n",
    "            preds.append(yhat_real)\n",
    "            trues.append(y_real)\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    return mean_squared_error(trues, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9ad0d",
   "metadata": {},
   "source": [
    "Ранняя остановка\n",
    "Класс EarlyStopper: ранняя остановка по метрике валидации с параметрами patience и min_delta. Если улучшений нет заданное число эпох, обучение прерывается, запоминается лучшая метрика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed3868b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience: int = 20, min_delta: float = 0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = np.inf\n",
    "        self.count = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, value, model):\n",
    "        if value < self.best - self.min_delta:\n",
    "            self.best = value\n",
    "            self.count = 0\n",
    "            self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        else:\n",
    "            self.count += 1\n",
    "            return self.count > self.patience\n",
    "\n",
    "    def load_best(self, model):\n",
    "        if self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156dd3b8",
   "metadata": {},
   "source": [
    "Функция обучения\n",
    "train_model(...): создаёт ResidualMLP, лосс MSE, выбирает оптимизатор (AdamW или SAM) и планировщик OneCycleLR. Цикл обучения: train-шаги, step шедулера, оценка на валидации (в реальном масштабе), логирование истории, ранняя остановка, сохранение лучшей модели. Возвращает (best_val_mse, history, best_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d76ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(optimizer_name: str = \"AdamW\", epochs: int = 100,\n",
    "                sam_rho: float = 0.05, sam_adaptive: bool = True):\n",
    "\n",
    "    model = ResidualMLP(in_dim=X_tr.shape[1]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if optimizer_name == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=cfg.lr, pct_start=cfg.pct_start,\n",
    "            steps_per_epoch=len(dl_tr), epochs=epochs,\n",
    "            div_factor=10.0, final_div_factor=1e3,\n",
    "        )\n",
    "    elif optimizer_name == \"SAM\":\n",
    "        optimizer = SAM(\n",
    "            model.parameters(),\n",
    "            base_optimizer=optim.AdamW,\n",
    "            lr=cfg.lr, weight_decay=cfg.weight_decay,\n",
    "            rho=sam_rho, adaptive=sam_adaptive,\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer.base_optimizer,        \n",
    "            max_lr=cfg.lr, pct_start=cfg.pct_start,\n",
    "            steps_per_epoch=len(dl_tr), epochs=epochs,\n",
    "            div_factor=10.0, final_div_factor=1e3,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"optimizer_name должен быть 'AdamW' или 'SAM'\")\n",
    "\n",
    "    history = []\n",
    "    early = EarlyStopper(patience=cfg.patience, min_delta=0.0)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "\n",
    "        for xb, yb in dl_tr:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            if optimizer_name == \"AdamW\":\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                yhat = model(xb)\n",
    "                loss = criterion(yhat, yb)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            else:  # SAM\n",
    "                # step 1\n",
    "                optimizer.base_optimizer.zero_grad(set_to_none=True)\n",
    "                yhat = model(xb)\n",
    "                loss = criterion(yhat, yb)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "\n",
    "                # step 2\n",
    "                yhat2 = model(xb)\n",
    "                loss2 = criterion(yhat2, yb)\n",
    "                loss2.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "\n",
    "            running += loss.item() * xb.size(0)\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss = running / len(ds_tr)\n",
    "        val_mse = evaluate_mse_real_scale(model, dl_val, y_scaler)\n",
    "        history.append({\"epoch\": epoch, \"train_mse_z\": train_loss, \"val_mse_year\": val_mse})\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(f\"[{optimizer_name}] Epoch {epoch:03d}/{epochs} | train(z): {train_loss:.4f} | val_MSE(year): {val_mse:.3f}\")\n",
    "\n",
    "        if early.step(val_mse, model):\n",
    "            print(f\"[{optimizer_name}] Early stop @ {epoch}, best val_MSE: {early.best:.3f}\")\n",
    "            break\n",
    "\n",
    "    early.load_best(model)\n",
    "    best_val = early.best\n",
    "    return best_val, history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac89ada",
   "metadata": {},
   "source": [
    "Запуск экспериментов и сабмит\n",
    "Две серии обучения: с AdamW и c SAM (заданы rho, adaptive). Сравнение лучших MSE, выбор лучшей модели. Затем инференс на тесте, обратное масштабирование y, формирование submission.csv (id, year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56b090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AdamW] Epoch 001/100 | train(z): 1.1672 | val_MSE(year): 104.055\n",
      "[AdamW] Epoch 010/100 | train(z): 0.5893 | val_MSE(year): 85.300\n",
      "[AdamW] Epoch 020/100 | train(z): 0.3533 | val_MSE(year): 102.533\n",
      "[AdamW] Early stop @ 28, best val_MSE: 84.313\n",
      "[SAM] Epoch 001/100 | train(z): 1.2102 | val_MSE(year): 106.432\n",
      "[SAM] Epoch 010/100 | train(z): 0.5899 | val_MSE(year): 86.045\n",
      "[SAM] Epoch 020/100 | train(z): 0.3492 | val_MSE(year): 93.790\n",
      "[SAM] Early stop @ 28, best val_MSE: 81.841\n",
      "\n",
      "==============================\n",
      "AdamW   val_MSE: 84.313\n",
      "SAM     val_MSE: 81.841\n",
      "==============================\n",
      "Saved -> submission.csv\n"
     ]
    }
   ],
   "source": [
    "val_mse_adam, hist_adam, model_adam = train_model(\"AdamW\", epochs=cfg.epochs)\n",
    "val_mse_sam,  hist_sam,  model_sam  = train_model(\"SAM\",   epochs=cfg.epochs, sam_rho=0.05, sam_adaptive=True)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"AdamW   val_MSE: {val_mse_adam:.3f}\")\n",
    "print(f\"SAM     val_MSE: {val_mse_sam:.3f}\")\n",
    "print(\"==============================\")\n",
    "\n",
    "# inference (на лучшей модели по валидации)\n",
    "best_model = model_sam if val_mse_sam <= val_mse_adam else model_adam\n",
    "best_model.eval()\n",
    "\n",
    "preds_test = []\n",
    "with torch.no_grad():\n",
    "    for xb in dl_te:\n",
    "        xb = xb.to(device)\n",
    "        yhat = best_model(xb).detach().cpu().numpy()\n",
    "        preds_test.append(yhat)\n",
    "\n",
    "preds_test = np.concatenate(preds_test)\n",
    "preds_year = y_scaler.inverse_transform(preds_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": X_test.index,\n",
    "    \"year\": preds_year.round().astype(int) \n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved -> submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
